# Optimization Models in Machine Learning: Introduction and Examples

This tutorial illustrates how many popular machine learning models can be formulated and solved through the lens of optimization.
Through a combination of slides and exercises, we present intuition behind gradient descent, linear and logistic regression, regularization, and outlier detection.

Prepared and presented by Sasha Aravkin, Maryam Fazel, Kelsey Maass, and Prasanna Raut.
Sponsored by the [Algorithmic Foundations of Data Science Institute (ADSI)](http://ads-institute.uw.edu/).
We acknowledge the support of NSF via the awards TRIPODS+X 1839291 and TRIPODS 1740551.

Notebooks:
* [Part 1 - Iterative Methods](notebooks/Part1-IterativeMethods.ipynb): Introduces gradient descent and stochastic gradient descent
* [Part 2 - Linear Regression](notebooks/Part2-LinearRegression.ipynb): Introduces linear regression and regularization, with a focus on the Lasso problem
* [Part 3 - Simple Logistic Regression](notebooks/Part3-SimpleLogisticRegression.ipynb): Introduces logistic regression
* [Part 4 - Logistic Regression](notebooks/Part4-LogisticRegression.ipynb): Introduces trimming for outlier detection and removal, applied to the logistic regression problem
